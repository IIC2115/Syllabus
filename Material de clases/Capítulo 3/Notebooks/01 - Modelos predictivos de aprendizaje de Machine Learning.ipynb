{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC2115 - Programación como herramienta para la ingeniería</font><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos predictivos basados en aprendizaje de máquina\n",
    "\n",
    "Uno de los objetivos principales del análisis de datos es la construcción de modelos predictivos basados en datos, que permitan inferir el comportamiento de un fenómeno en el futuro. En este capítulo, nos centraremos en técnicas de aprendizaje de máquina para construir estos modelos predictivos basados en datos. Para esto, utilizaremos la librería `scikit-learn`, que la más utilizada para este tipo de análisis.\n",
    "\n",
    "Antes de describir los modelos y el uso de la librería, presentaremos una breve introducción a los conceptos teóricos esenciales del aprendizaje de máquina. En particular, describiremos los distintos tipos de aprendizaje que existen, el mecanismo de entrenamiento de un modelo y como se evalúa su comportamiento a través conjuntos de validación y métricas de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es el aprendizaje de máquina?\n",
    "\n",
    "El aprendizaje de máquina, o _Machine Learning_ (ML) en inglés, es una disciplina que combina conocimientos y técnicas de ciencia de la computación, cálculo y estadística, donde se busca construir modelos de análisis y procesamiento de datos que permitan resolver problemas específicos. Estos modelos cumplen en general las siguientes características:\n",
    "* Modelos (principalmente) predictivos basados en datos, enfocados en problemas específicos.\n",
    "* Mejoran con la experiencia/datos (generalmente, mientras más, mejor).\n",
    "* Buscan aprendizaje más que modelamiento de datos.\n",
    "\n",
    "Estas características, unidas a su buen rendimiento en problemas reales y a la gran cantidad de librerías relacionadas disponibles, hacen que las técnicas de _machine learning_ sean ampliamente usadas para analizar datos. Algunos ejemplos de uso del aprendizaje de máquina son los detectores automáticos de fraude bancario, los filtros anti-spam, los sistemas recomendadores y los sistemas de traducción entre idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de aprendizaje de máquina\n",
    "\n",
    "Si bien es posible definir más tipos, en general las técnicas y modelos de _machine learning_ pueden corresponder a tres tipos distintos de aprendizaje: supervisado, no supervisado y reforzado: \n",
    "\n",
    "<img src=\"figs/ml_categories.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "El aprendizaje supervisado es el más común e incluye problemas como regresión y clasificación. En este esquema, el objetivo es predecir para cada dato ya sea un valor numérico (regresión) o una etiqueta (clasificación). Para lograr esto, se cuenta con un conjunto de datos donde estos valores son conocidos, a partir de los cuales estima un modelo predictivo que idealmente sea capaz de generalizar a datos no vistos anteriormente. Por ejemplo, si los datos consisten en información de las condiciones meteorológicas, incluida la temperatura, para todos los días del año 2020, podríamos construir un modelo de aprendizaje supervisado que, dadas las condiciones meteorológicas de cualquier día del año 2021, prediga la temperatura para el día siguiente.\n",
    "\n",
    "El aprendizaje no supervisado se centra en el descubrimiento de estructuras o relaciones en los datos, sin contar con etiquetas o valores a predecir, como en el aprendizaje supervisado. Ejemplos de tareas de aprendizaje no supervisado son el _clustering_ y la reducción de dimensionalidad.\n",
    "\n",
    "Finalmente, el aprendizaje reforzado considera la existencia de un *agente* que debe interactuar con un *entorno*, con el fin de cumplir una tarea. A diferencia del aprendizaje supervisado, no existen etiquetas que indiquen lo que se debe hacer, sino que el agente recibe una recompensa (relacionada con la tarea) al interactuar con el ambiente, y así este buscará maximizar esta recompensa en el tiempo. Ejemplos de dominios donde se aplica el aprendizaje reforzado son el control de personajes en juegos o los procesos de decisión con incerteza.\n",
    "\n",
    "En este curso nos centraremos en los modelos y técnicas de aprendizaje supervisado y no supervisado. Si está interesado en aprender más sobre estos y otros tópicos relacionados, se recomienda tomar en el futuro cursos como Inteligencia Artificial (IIC2613) o Sistemas Urbanos Inteligentes (ICT3115)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos\n",
    "\n",
    "Como norma general, los modelo de _machine learning_ trabajan sobre datos multidimensionales, es decir, cada dato es considerado como un vector multidimensional, donde cada dimensión corresponde a un atributo o variable. Por ejemplo, si tenemos un `DataFrame`, podemos considerar que cada fila es un vector, donde cada columna representa una de las dimensiones del dato. Si bien este es un caso común, los modelos de ML pueden trabajar con otro tipo de datos multidimensionales, como imágenes o grafos. Para referirse al espacio vectorial donde viven los datos, generalmente se utiliza el término espacio de característica, o _feature space_.\n",
    "\n",
    "La estimación de los parámetros de un modelo se realiza a través de un proceso llamado **entrenamiento**. En él, los datos son divididos en dos conjuntos disjuntos: conjunto (o set) de entrenamiento, y conjunto (o set) de prueba (o test). Los detalles de cómo exactamente se entrena cada modelo exceden al alcance del curso, pero todos siguen aproximadamente el siguiente proceso. Dado que el set de entrenamiento en un conjunto acotado de datos, se asumen inicialmente que este es suficientemente representativo de la distribución real de los datos. Dado esto, se itera sobre este conjunto de manera que las predicciones hechas por el modelo sean progresivamente mejores, es decir, más cercanas a las etiquetas o valores a predecir. Para muchos modelos, este proceso de mejora iterativa es planteado y resuelto como un problema de optimización, donde se debe minimizar una función de pérdida o costo, que mide la discrepancia entre las predicciones y los valores originales a predecir. \n",
    "\n",
    "A diferencia de otros modelos estadísticos, lo modelos de ML consideran la evaluación de su rendimiento a través del cálculo de una métrica de error (o éxito) en el conjunto de test. Dado que los ejemplos en el set de test no fueron utilizados en el entrenamiento, un alto rendimiento del modelo en él asegura que este es capaz de generalizar a casos no vistos previamente, lo que da garantía de su robustez.\n",
    "\n",
    "Otro de los motivos por el cual se utiliza un set de datos independiente para mejor el rendimiento, es debido a un fenómeno conocido como **sobreajuste** u _overfitting_, en donde el modelo memoriza los ejemplos que procesó en el set de entrenamiento, lo que redunda en un rendimiento casi perfecto en este conjunto de datos. Sin embargo, al evaluar el rendimiento en el set de prueba, este suele ser notoriamente menor que el obtenido en el set de entrenamiento. Esta situación se genera la mayoría de las veces por: i) realizar una cantidad excesiva de iteraciones de entrenamiento y/o ii) por tener un modelo de mayor complejidad a la necesaria para resolver el problema (sobreparametrizado). La siguiente figura muestra gráficamente la divisiones realizadas en un set de datos para generar los conjuntos de entrenamiento y test:\n",
    "\n",
    "<img src=\"figs/datasets.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "Finalmente, tal como existe sobreajuste, es posible tener problemas de subajuste o _underfitting_, donde la capacidad del modelo no es suficiente para capturar el fenómeno a predecir. Un caso típico de _underfitting_ y _overfitting_ se puede apreciar en la siguiente figura, donde se muestra el resultado de estimar una función cuadrática donde los datos tiene ruido:\n",
    "\n",
    "Underfitting | Good fit | Overfitting\n",
    ":-: | :-: | :-:\n",
    "<img src=\"figs/underfitting.png\" width=\"300px\" align=\"center\"/> | <img src=\"figs/good_fit.png\" width=\"300px\" align=\"center\"/> | <img src=\"figs/overfitting.png\" width=\"270px\" align=\"center\"/>\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de aprendizaje supervisado\n",
    "\n",
    "Como se mencionó anteriormente, existen múltiples modelos de aprendizaje supervisado, cada uno con sus características propias, ventajas y desventajas que los hacen más o menos adecuados a los disintos tipos de problemas de datos que encontremos. A continuación se describirán las características de algunos de ellos.\n",
    "\n",
    "### K-vecinos cercanos (KNN)\n",
    "Este algoritmo consiste en realizar predicciones basadas unicamente en los K elementos más cercanos al dato analizado, en el espacio de características. Si la tarea es una clasificación, el resultado se entrega a través de una votación de mayoría (las clase más repetida entre los K vecinos gana) o un promedio si la tarea es una regresion (promedio de los K vecinos). Al solo basarse en el cálculo de distancias, este modelo no necesita entrenamiento, lo que lo hace muy simple y rápido de utilizar. Sin embargo, su rendimiento en cuanto a tiempo de ejecución sufre mucho cuando la cantidad de ejemplos en el set de entrenamiento es alta (piense en la complejidad computación de calcular la distancia euclidiana entre el punto analizado y todos los ejemplos de entrenamiento y luego ordenarlas). Otra desventaja es la sensibilidad al hiperparámetro K, que altera fuertemente el rendimiento.\n",
    "\n",
    "<img src=\"figs/knn.png\" width=\"300px\" align=\"center\"/>\n",
    "\n",
    "### Regresión lineal, logística y polinomial\n",
    "Estas técnicas de aprendizaje estadístico permiten estimar funciones (reg. lineal) o clasificar (reg. logística) en base a una combinación lineal de las características de los datos. Además, si se expanden las características utilizando una transformación polinomial de estas, es posible hacer estimaciones de funciones y clasificación no lineal. Estas técnicas son ampliamente utilizadas debido a su rapidez, sencillez e interpretabilidad, pero tienen problemas para obtener buenos rendimientos en problemas complejos.\n",
    "\n",
    "Regresión lineal | Regresión logística | Regresión polinomial\n",
    ":-: | :-: | :-:\n",
    "<img src=\"figs/linear_regression.png\" width=\"300px\" align=\"center\"/> | <img src=\"figs/logistic_regression.png\" width=\"290px\" align=\"center\"/> | <img src=\"figs/polynomial_regression.png\" width=\"250px\" align=\"center\"/>\n",
    "\n",
    " \n",
    "\n",
    "### Support Vector Machine\n",
    "Un Support Vector Machines (SVM) permite hacer clasificación binaria o multiclase, utilizando el concepto de máximo margen. En él, se busca el clasificador lineal (hiperplano) que maximice la distancia entre las clases. Esto permite obtener un poder de generalización más alta que otros métodos como la regresión lineal. Además del rendimiento, los SVM son muy rápidos de entrenar, lo que los hace ser candidatos perfectos cuando se tiene gran volumen de datos.\n",
    "\n",
    "<img src=\"figs/svm.png\" width=\"250px\" align=\"center\"/>\n",
    "\n",
    "### Árboles de decisión y regresión\n",
    "Los árboles de decisión son técnicas simples de clasificación o regresión, que funcionan con cualquier tipo de atributo (numéricos, categóricos, binarios, etc). Su funcionamiento se basa en la construcción  de una estructura de árbol, en base a tests/umbrales calculados para cada característica analizada. Esta árbol es construido hasta llegar a los nodos hoja, donde se entrega la predicción o clasificación. Si bien sufren serios problemas de _overfitting_, su alto nivel de explicabilidad los hace siempre buenos candidatos, sobre todo si se tiene gran cantidad de datos.\n",
    "\n",
    "<img src=\"figs/decision_tree.png\" width=\"300px\" align=\"center\"/>\n",
    "\n",
    "### Ensambles de árboles\n",
    "Lás técnicas de ensamble se basan en la combinación de múltiples árboles para generar una predicción más robusta y menos sesgada. Si bien existen múltiples existen diversas maneras de combinar estas predicciones, las estrategias paralela aleatoria (_Random Forest_) y secuencial (_Gradient Boosting_) son las que mejores resultados obtienen en la práctica. Estas técnicas son en la actualidad de las más competiticas en cuanto a rendimiento al utilizar datos tabulados.\n",
    "\n",
    "<img src=\"figs/ensemble.png\" width=\"330px\" align=\"center\"/>\n",
    "\n",
    "### Redes Neuronales\n",
    "Las redes neuronales son modelos altamente generales y complejos para estimar funciones de cualquier tipo (de hecho, es posible demostrar que pueden aprender cualquier función). Su principal característica es que procesan los datos a través de varias capas, que combinan operaciones lineales y no lineales. Si se tienen muchos datos, estos modelos son por lejos los que mejor funcionan, en particular para datos no estructurados (imágenes, texto, audio, video, etc.)\n",
    "\n",
    "<img src=\"figs/nn.png\" width=\"300px\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de aprendizaje no supervisado\n",
    "\n",
    "Típicamente, las técnicas de aprendizaje no supervisado se asocian con dos tareas, el _clustering_ y la reducción de dimensionalidad. A continuación, se describirán las características de algunos de los algoritmos típicos para enfrentar estas tareas.\n",
    "\n",
    "### Algoritmos de clustering\n",
    "Se utilizan principalmente para dividir un conjunto de datos en grupos (llamados ``clústers'') de forma tal que los elementos de un clúster sean similares entre sí. Las técnicas de clustering son útiles en muchos contextos, como la segmentación de clientes o la agrupación de documentos por tema. Los dos esquemas de clustering más comunes son el esquema jerárquico y el de centroide. \n",
    "\n",
    "#### Esquemas jerárquicos\n",
    "Los esquemas jerárquicos se basan en la construcción de una estructura jerárquica de relación entre los ejemplos, en base a la idea de que ejemplos cercanos en el espacio de características están más relacionados entre sí que los que están más separados. El algoritmo de clustering jerárquico más importante es el Agglomerative Hierarchical Clustering, o AHC, que trabaja uniendo puntos uno a uno en base a su distancia. Este algoritmo es muy popular debido a su simplicidad de funcionamiento e implementación, además de que no requiere de la especificación previa de la cantidad de clústers que se generarán. Por otro lado, el algoritmo presenta dificultades para definir el umbral de corte preciso, lo que además dificulta el manejo de outliers, ya que pueden incluirse fácilmente en un clúster ya existente si no se considera un umbral especial para ellos.\n",
    "\n",
    "<img src=\"figs/ahc.png\" width=\"600px\" align=\"center\"/>\n",
    "\n",
    "#### Esquemas de centroide\n",
    "Los esquemas de centroide se basan en la idea de que cada clúster es representado por un vector de características, que actúa como el centroide o la media de los elementos que lo componen. Luego, la pertenencia un ejemplo a un clúster se define en base a la distancia con respecto a este centroide en el espacio de características. El algoritmo más importante para este modelo de clúster, y quizá el algoritmo de clustering más popular en general, es K-means. Este algoritmo divide el conjunto de datos en un número predefinido (K) de clústers, usando típicamente como métrica de distancia a la distancia euclidiana. El representante de cada grupo se llama centroide y corresponde al vector de características promedio de los ejemplos asignados a ese clúster. Si bien funcionamiento simple e intuitivo de K-means hace que sea muy popular, tiene la gran desventaja de que es necesario seleccionar por adelantado el número K de clústers. Para llevar a cabo esto, se utiliza típicamente el método _Silhouette_ o el método del codo. \n",
    "\n",
    "<img src=\"figs/kmeans.png\" width=\"600px\" align=\"center\"/>\n",
    "\n",
    "### Reducción de dimensionalidad\n",
    "Independiente del paradigma de aprendizaje que se utilice, en Machine Learning los datos forman la base de cualquier algoritmo de aprendizaje. Por este motivo, las características que se utilizan para modelar los datos idealmente deben capturar los atributos fundamentales de estos, con el fin de facilitar el trabajo de los algoritmos. Sin embargo, muchas veces el número de características resulta ser excesivo, ya sea debido a redundancia o que algunas de las cuales derechamente ni siquiera son afines a los datos. Este exceso de características puede no solo complicar a los modelos de aprendizaje con respecto a su posibilidad de sobreajuste, sino que además hacen más difícil, la interpretación y comprensión de los datos y los resultados a través de su visualización. Aquí es donde entran en juego las técnicas de reducción de la dimensionalidad.\n",
    "\n",
    "La reducción de dimensionalidad es la tarea de reducir la cantidad de características en un conjunto de datos. El proceso de reducción esencialmente transforma los datos de un espacio de características de alta dimensión a uno de baja dimensión, cuidando que las propiedades más significativas presentes en los datos no se pierdan durante la transformación.\n",
    "\n",
    "Es importante destacar que la tarea de reducción de dimensionalidad es distinta de la de selección de características. Si bien ambas generan como resultado un número de características menor al original, la selección de características elige aquellas que mejor permiten resolver una tarea predictiva en particular, mientras que la de reducción crea nuevas características a partir de las originales, en base a los atributos intrínsecos de los datos, no necesitando así la definición explícita de una tarea de predicción.\n",
    "\n",
    "<img src=\"figs/dimensionality_reduction.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "Existen dos familias principales de algoritmos de reducción de dimensionalidad, los de proyección lineal y los de aprendizaje de manifold. A continuación, describiremos ambas familias, junto con sus algoritmos más representativos.\n",
    "\n",
    "#### Algoritmos de proyección lineal\n",
    "Como su nombre lo indica, estos algoritmos transforman los datos a través de la aplicación de operaciones lineales, de modo que la proyección cumpla ciertas condiciones. El algoritmo más utilizado de esta familia es el _Principal Component Analysis_ o PCA, el cual busca proyectar los datos a un espacio de menor dimensionalidad, preservando la varianza presente en ellos. Para esto, es necesario buscar los componentes principales de los datos, que corresponden a los vectores y valores propios de la matriz de características de estos. Estos vectores definirán los ejes del nuevo espacio de características, donde aquellos con mayores valores propios capturarán la mayor cantidad de varianza de los datos. \n",
    "\n",
    "De esta forma, es posible reducir la dimensionalidad proyectando los datos a un espacio con una cantidad de ejes menor a la original, multiplicando la matriz de características por los una fracción de los vectores propios encontrados, en orden de mayor a menor captura de varianza. Al igual que todos los algoritmos de esta familia, PCA tiene problemas para proyectar datos cuyas características presenten relaciones no lineales.\n",
    "\n",
    "#### Algoritmos de aprendizaje de manifold\n",
    "Los algoritmos de aprendizaje de manifold evitan las limitantes de los algoritmos de proyección lineal, al utilizar transformaciones no lineales para mover los datos del espacio de origen a un manifold de destino, que mantiene las propiedades locales de los ejemplos. Uno de los algoritmos de esta familia más utilizados es el _t-Distributed Stochastic Neighbor Embedding_ o t-SNE, que es muy adecuada además para la visualización de datos. A diferencia de PCA, que simplemente maximiza la captura de la varianza de los datos, t-SNE minimiza la divergencia entre distribuciones de datos. Esencialmente, captura localmente la distribución de los en el espacio de alta dimensión y luego aprende una proyección no lineal que mantenga lo más similar posible esta distribución, pero en el espacio de baja dimensionalidad. Esto último se logra a través del aprendizaje de una proyección no lineal, mediante un problema de minimización, donde la función objetivo corresponde a la divergencia KL. Un detalle importante de los algoritmos de esta familia es su gran demanda por recursos computacionales, lo que lleva a utilizar estrategias combinadas cuando los espacios de origen son de muy alta dimensionalidad. Específicamente, se recomienda primero aplicar PCA para llevar los datos a un espacio de menor dimensionalidad, menor o igual a 50 dimensiones por ejemplo, y luego aplicar t-SNE. \n",
    "\n",
    "Para concluir, podemos realizar una comparación visual de estos algoritmos, para hacernos una idea de sus diferencias:\n",
    "\n",
    "<img src=\"figs/pca_vs_tsne.png\" width=\"600px\" align=\"center\"/>\n",
    "\n",
    "Al reducir a 2 dimensiones el espacio de características del conjunto de datos MNIST, que consiste en imágenes de 28x28 pixeles de dígitos entre 0 y 9 escritos a manos, podemos notar claras diferencias en el resultado. Solo con el fin de facilitar la visualización, se han coloreado los puntos con un color correspondiente a la etiqueta que poseen, sin haber utilizado esa información en la reducción de dimensionalidad. Es posible apreciar que si bien todos los algoritmos capturan grupos que coinciden con las etiquetas, los resultados de t-SNE son mucho más satisfactorios, separando de mejor manera los puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de aprendizaje de máquina en Python\n",
    "\n",
    "Como se mencionó anteriorment, utilizaremos la librería `scikit-learn` para aplicar técnicas de ML al procesamiento de datos. La popularidad de esta librería radica principalmente en el uso de una interfaz limpia, uniforme y simple, que facilita la exploración y permite la integración con otras librerías, como Pandas. En otras palabras, todos los modelos y técnicas disponibles funcionan _out-of-the-box_ y utilizando una interfaz de ejecución y configuración casi idéntica para todos. Esto permite además obviar parcialmente los detalles teóricos de los modelos y concentrarse en la exploración y construcción de soluciones.\n",
    "\n",
    "La librería ya se encuentra instalada en Colab, por lo que solo es necesario importar sus componentes. En caso de no utilizar Colab, el siguiente comando puede ser utilizado para la instalación: `!pip install sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esquema de representación de datos\n",
    "El esquema de representación de datos de `scikit-learn` es sumamente directo y sencillo. Salvo algunos casos particulares, todos los modelos recibirán como entrada para entrenamiento y evaluación, una matriz de features y un vector objetivo. La matriz de features, o **X**, tiene dimensiones `n_ejemplos X n_features` y puede ser representada por un `DataFrame` (o un `array` de `Numpy`). El vector objetivo, o **y**, contiene los valores a predecir para cada ejemplo (sean estos números o categorías), tiene dimensiones de `n_ejemplos X 1` y puede ser representado por una `Series` (o un `array` de `Numpy`).\n",
    "\n",
    "Dependiendo del caso, la matriz *X* podrá representará el set de entrenamiento, de validación o de test, pero siempre tendrá el mismo formato.\n",
    "\n",
    "<img src=\"figs/sklearn_data_model.png\" width=\"400px\" align=\"center\"/>\n",
    "\n",
    "El siguiente ejemplo muestra como cargar un conjunto de datos y luego crear sets de datos independientes para entrenamiento, test y validación, utilizando las funcionalidades de `scikit-learn`. En particular, utilizaremos el conjunto de datos [`Iris`](https://archive.ics.uci.edu/ml/datasets/iris). Este conjunto de datos es ideal para practicar conceptos de ML, debido a su tamaño y simplicidad. El set `Iris` contiene 150 ejemplos pertenecientes a 3 clases de plantas de la familia Iris, donde cada ejemplo está descrito por 4 características. El conjunto de datos se encuentra disponible en el sitio del curso y debe ubicarse en la misma carpeta que el código para que la siguiente celda funcione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "iris_dataset = pd.read_csv('iris.csv')\n",
    "iris_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de poder construir los modelos predictivos, es necesario crear la matriz **X** y el vector **y** para cada caso. Además de esto, `scikit-learn` requiere que todas las _features_ sean numéricas, por lo que de deben transformar todas las variables categóricas en el conjunto de datos a numéricas (en este caso solo `species`), codificando las categorías como números naturales. Esto se puede hacer usando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_vars = ['species']\n",
    "label_encoder = LabelEncoder()\n",
    "for i in cat_vars:\n",
    "    iris_dataset[i] = label_encoder.fit_transform(iris_dataset[i])\n",
    "iris_dataset.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante que el paso anterior se realice siempre antes de la creación de los conjuntos de entrenamiento, validación y test. De esta manera, se puede asegurar la consistencia en la codificación de las variables categóricas para todos los conjuntos (si no, por ejemplo, podría ser que la especie _setosa_ en uno de los conjuntos sea codificada como un 1 y en otro como un 0).\n",
    "\n",
    "A continuación, se utiliza la función `train_test_split` de `scikit-learn` para crear los conjuntos de datos respectivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, test_set = train_test_split(iris_dataset.copy(), test_size = 0.3)\n",
    "\n",
    "print(f'Tamaño set entrenamiento: {len(training_set)}')\n",
    "print(f'Tamaño set test: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible, y muchas veces recomendable, realizar una etapa de preprocesamiento de los datos antes de entregarlos a los modelos predictivos. Por ejemplo, es común normalizar cada _feature_ con el fin de que todas se encuentren en la misma escala. Sin embargo, este proceso requiere bastante cuidado ya que es muy fácil caer en el problema de _data leakage_, que consiste en incluir de manera indirecta en el set de entrenamiento información que pertenece al set de test. Un ejemplo típico de esto es usar un proceso de estandarización para normalizar las _features_, calculando la media y varianza en base al conjunto completo de los datos (en este caso `iris_dataset`), incluyendo de manera indirecta información del futuro conjunto de prueba, en el futuro conjunto de entrenamiento. Un procedimiento adecuado es calcular estos coeficientes en el conjunto de entrenamiento (en este caso `training_set`) con el comando `fit_transform`y luego usarlos para estandarizar los datos en el conjunto de test, con el comando `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features = ['sepal_length','sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "training_set[features] = scaler.fit_transform(training_set[features])\n",
    "test_set[features] = scaler.transform(test_set[features])\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis exploratorio\n",
    "\n",
    "Con el fin de darnos una idea de las particularidad de los datos y de la dificultad de la tarea de clasificación asociada, realizaremos un análisis exploratorio, que considera inicialmente visualizaciones a través de reducción de dimensionalidad. Específicamente, utilizando PCA y t-TSNE, reduciremos el espacio de características del conjunto de entrenamiento de 4 a 2 dimensiones para hacerlo humanamente entendible, y visualizaremos los resultados utilizando un color distinto para cada valor de la etiqueta. Recordemos que al ser esquemas de aprendizaje supervisado, ninguno de los dos algoritmos utiliza la información de la etiqueta en su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "target_names = label_encoder.inverse_transform([0, 1, 2])\n",
    "\n",
    "X_PCA = PCA(n_components=2).fit_transform(training_set[features])\n",
    "X_TSNE = TSNE(n_components=2).fit_transform(training_set[features])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_PCA[training_set['species'] == i, 0], X_PCA[training_set['species'] == i, 1], color=color, alpha=0.8, lw=2, label=target_name)\n",
    "#plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"Transformación PCA del conjunto IRIS\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_TSNE[training_set['species'] == i, 0], X_TSNE[training_set['species'] == i, 1], color=color, alpha=0.8, lw=2, label=target_name)\n",
    "plt.title(\"Transformación TSNE del conjunto IRIS\")\n",
    "\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos apreciar, ambos esquemas de reducción de dimensionalidad nos muestran la existencia de 3 grupos claramente separados, los cuales corresponden a las tres categorías del conjunto. Podemos notar ademas la existencia de algunos ejemplos difíciles de separar, ya que se encuentran mezclados con puntos de otro color.\n",
    "\n",
    "A continuación, para validar numéricamente lo visualizado, probaremos múltiples valores del parámetro K del algoritmo de clustering K-means, con el fin de encontrar aquel que mejor captura la estructura de los datos. Idealmente, este número debiera coincidir con la cantidad de categorías reales de los datos, en este caso 3. Sin embargo, es posible que existan clústers dentro de cada categoría, lo que haría que el número de centroides óptimo difiera de la cantidad de categorías indicada en la etiqueta de los datos.\n",
    "\n",
    "Para el análisis utilizaremos la varianza intra-cluster como métrica de ajuste , que se obtiene al calcular, dentro de cada clúster, la suma del cuadrado de las distancias entre su centroide y cada uno de los puntos del clúster. Esta métrica es una medida de la variabilidad de las observaciones dentro de cada clúster y, en general, un clúster que tiene un valor pequeño es más compacto que un clúster que tiene uno grande.\n",
    "junto con la metodología de el codo para encontrar el número óptimo de centroides que capturan los distintos grupos en los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cluster_var = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(training_set[features])\n",
    "    cluster_var.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "plt.plot(range(1, 11), cluster_var)\n",
    "plt.title('Método del codo')\n",
    "plt.xlabel('Número de clústers K')\n",
    "plt.ylabel('Varianza intra-clúster')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar el valor óptimo, utilizaremos una heurística llamada _el método del codo_, que consiste en graficar la varianza intra-clúster para distintos valores de K e identificar el punto \"codo\", donde la tasa de descenso de la varianza disminuye bruscamente, lo que sugiere que un mayor número de clúster no generará una mejora sustantiva en la métrica de ajuste. En este caso, podemos notar como a partir de K=3, la varianza intra-clúster comienza a descender de manera suave, lo que sugiere que el valor óptimo es ese.\n",
    "\n",
    "Como ejercicio propuesto, queda la aplicación de K-means a los datos proyectados mediante PCA y TSNE, con el fin de facilitar la visualización de los centroides encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción de modelos predictivos\n",
    "\n",
    "Con los datos ya preparados, visualizados y analizados, podemos empezar el proceso de entrenamiento de los modelos predictivos. Para facilitar esto, definiremos una función que realice entrenamiento y evaluación de rendimiento en un set separado, para cualquier modelo que se le entregue como parámetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def training_and_eval(model, training, eval, features, target):\n",
    "    model.fit(training[features], training[target])\n",
    "    predictions = model.predict(eval[features])\n",
    "    accuracy = metrics.accuracy_score(predictions, eval[target])\n",
    "    print(f\"Accuracy: {accuracy: .2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica de rendimiento utilizada en este caso es _accuracy_, que corresponde a la razón entre aciertos y la cantidad de ejemplos procesados. Cuando las clases no están balanceadas, se recomienda utilizar la métrica _balanced accuracy_. Queda como ejercicio propuesto realizar este ejercicio para una tarea de regresión.\n",
    "\n",
    "Teniendo todo esto, podemos comenzar a probar distintos modelos. Para este ejemplo evaluaremos 3: árbol de decisión, SVM y KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "target = 'species'\n",
    "model = DecisionTreeClassifier()\n",
    "training_and_eval(model, training_set, test_set, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "training_and_eval(model, training_set, test_set, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "training_and_eval(model, training_set, test_set, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el set de datos es pequeño y simple, el rendimiento de todos los modelos es excelente. En vista de eso, queda como ejercicio propuesto explorar más sets de datos (hay muchos en `scikit-learn`) y evaluar su rendimiento considerando el uso de un set de validación para controlar el _overfitting_. Para esto último, explore los distintos hiperparámetros que tiene cada modelo, con el fin de considerar como afectan estos a la complejidad de los modelos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
