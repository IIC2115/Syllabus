{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhlQiAcuyZJg"
   },
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC2115 - Programación como herramienta para la ingeniería</font><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgO72zcEyZJj"
   },
   "source": [
    "# Web Scraping\n",
    "\n",
    "En este capítulo haremos una introducción a ***Web Scraping***. Este es el proceso de extraer información de sitios web. El scraping de datos se enfoca en transformar el contenido no estructurado de un sitio web (usualmente HTML) en datos estructurados los que pueden ser almacenados en una base datos, en una hoja de cálculo o en el mismo código.\n",
    "\n",
    "La forma en que los datos son extraídos de un sitio web es similar a la utilizada por los bots de búsqueda - la navegación web humana es simulada utilizando programas (bots) los que extraen (scrape) los datos de un sitio web.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iirPjIRxyZJm"
   },
   "source": [
    "## Tipos de páginas Web\n",
    "\n",
    "A la hora de hacer scrapping en páginas web, es importante distinguir cómo está construida y alojada la información. Esta puede estar contenida directamente en el html (estática) o puede ser generada por alguna incrustación dinámica dentro del html como un javaScript (dinámica).\n",
    "\n",
    "La información estática es de más facil acceso ya se encuentra contenida directamente en el html y es en la que profundizaremos.\n",
    "\n",
    "Si necesita trabajar con páginas web deinámicas es recomendable investigar la librería `Selenium` de Python. Esta permite simular un usuario que interactúa en una página web donde se permite la recolección de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fZ7J7cAE0mA"
   },
   "source": [
    "## Aplicación con Beatiful Soup\n",
    "\n",
    "Para el caso de realizar Web Scraping en páginas que són estáticas se recomienda el uso de la librería `Beatiful Soup`. Con esta librería desarrollaremos el siguiente ejemplo. Suponga que se requiere desarrollar una aplicación que permita conocer el valor de la UTM en todo momento. Para conocer dicho valor podemos basarnos en el que publica la web \"prensa digital\" [aqui](https://www.prensadigital.cl/valor-utm-unidad-tributaria-mensual.html).\n",
    "\n",
    "### PASO 1: Conocer y familiarizarse con la web\n",
    "\n",
    "Es fuertemente recomendable que utilicen el navegador Google Chrome para abrir la url. Una vez abierta, debe seleccionar el valor del dolar y con click secundario selecciones \"inspeccionar elemento\". Ahora podrán ver el código de la pagina web y familiarizarse con ella.\n",
    "\n",
    "### PASO 2: Descargar el html\n",
    "\n",
    "Ahora utilizaremos la librería urllib para simular un navegador y descargar el código fuente de la página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UZiHSjnF-ED"
   },
   "outputs": [],
   "source": [
    "import urllib.request as net\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bicENIvg864V"
   },
   "outputs": [],
   "source": [
    "class WebDownloader:\n",
    "\n",
    "    def __init__(self, link):\n",
    "        self.user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "        self.url =  link\n",
    "\n",
    "\n",
    "    def getHtmlAsString(self):\n",
    "        headers = {'User-Agent':self.user_agent}\n",
    "        request= net.Request(self.url,None,headers)\n",
    "        gcontext = ssl.create_default_context() # Using create_default_context for better practice and to avoid deprecation warnings\n",
    "        response = net.urlopen(request,context=gcontext)\n",
    "        return response.read().decode('utf-8')\n",
    "\n",
    "wd = WebDownloader('https://www.prensadigital.cl/valor-utm-unidad-tributaria-mensual.html')\n",
    "sourceCode = wd.getHtmlAsString()\n",
    "print(sourceCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5UQRwn5CF2s"
   },
   "source": [
    "### PASO 3: Buscar el valor solicitado\n",
    "\n",
    "Al revisar el código fuente, es posible notar que el valor de la UTM se encuentra dentro del archivo que acabamos de descargar. En este caso basta con \"jugar\" con el string descargado para poder obtener el valor buscado.\n",
    "\n",
    "Pero existe una forma un poco más directa. Podemos inspeccionar el código fuente en el browser y seleccionar los elementos del sitio web que almacenan el valor buscado. En este caso, vemos que el valor de la UTM se presenta en múltiples partes del sitio. En este caso, utilizaremos específicamente el que se almacena en la tabla inferior, que se encuentra representado en el código de la siguiente manera:\n",
    "\n",
    "`<td>61.769</td>`\n",
    "\n",
    "esta dentro del nodo \"td\", en este caso podemos acceder a todos los nodos con ese nombre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Hry2-nmF_5Q"
   },
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDghNLnyAz1f"
   },
   "outputs": [],
   "source": [
    "info = []\n",
    "soup = bs4.BeautifulSoup(sourceCode)\n",
    "\n",
    "for node in soup.find_all('td'):\n",
    "    info.append(str(u''.join(node.find_all(string=True)).encode('utf-8'))[2:-1])\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht--g2-cKMtL"
   },
   "source": [
    "En este caso vemos como apuntaba a la tabla de la web.\n",
    "\n",
    "### PASO 4: Mostrar la información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DR65XjR9H0EG"
   },
   "outputs": [],
   "source": [
    "print(\"El valor actual de la UTM es: {}\".format(info[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQBrqKEtKk1_"
   },
   "source": [
    "Funciona!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_lq20UdKp2l"
   },
   "source": [
    "En general Web Scraping es a medida. Es decir, deben desarrollarse distintas herramientas en función de las necesidades. Documentarse correctamente es fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UobWfLtrz9u0"
   },
   "source": [
    "## Web Scraping con pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2pJIKDG0IvV"
   },
   "source": [
    "También podemos realizar un ejemplo básico de web scraping utilizando requests, BeautifulSoup y pandas. Primero descargamos el contenido HTML de una página de Wikipedia y usamos BeautifulSoup para encontrar la tabla que contiene los datos de comunas de Chile. Luego, pandas.read_html se encarga de parsear automáticamente la tabla HTML y convertirla en un DataFrame. Esta función internamente utiliza lxml o html5lib para interpretar el contenido estructurado de la tabla y extraerlo como datos tabulares, lo que simplifica enormemente el proceso de extracción de datos desde sitios web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDOQbTrIF3iK"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnAspK1Q0ChB"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "url = \"https://es.wikipedia.org/wiki/Anexo:Comunas_de_Chile\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Encontramos la tabla que contiene las comunas\n",
    "table = soup.find_all(\"table\", {\"class\": \"wikitable\"})[0]\n",
    "df = pd.read_html(StringIO(str(table)))[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01t1SxGm0Mo6"
   },
   "source": [
    "Este tipo de extracción es útil cuando se necesita recopilar información actualizada desde la web que aún no está disponible en formato descargable o API. Una vez obtenida la tabla como DataFrame, se pueden inspeccionar y depurar los datos, realizar análisis, visualizaciones o integrarla con otros conjuntos de datos. El web scraping permite así automatizar la recolección de información desde sitios públicos, facilitando tareas que de otro modo requerirían recopilación manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b23c8ea0"
   },
   "source": [
    "\n",
    "## Acceso a datos vía API (REST)\n",
    "\n",
    "Además de obtener información desde el HTML de una página web, otra forma eficiente y profesional de obtener datos es mediante una API (Application Programming Interface).\n",
    "\n",
    "Una **API** es un conjunto de reglas y especificaciones que permiten que dos aplicaciones se comuniquen entre sí. Muchas plataformas públicas y privadas exponen APIs para que podamos consultar sus datos de forma controlada, estructurada y actualizada.\n",
    "\n",
    "A diferencia del web scraping (que puede romperse si cambia el HTML), las APIs ofrecen estabilidad, formato consistente (generalmente JSON) y documentación oficial.\n",
    "\n",
    "### Tipos de APIs comunes\n",
    "- **REST APIs**: accedidas vía URLs y usando métodos HTTP como GET, POST.\n",
    "- **GraphQL APIs**: consultas flexibles en un solo endpoint.\n",
    "- **APIs privadas**: requieren autenticación más estricta, a veces vía OAuth.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UylOl_W58RoF"
   },
   "source": [
    "### ¿Cómo funcionan las APIs REST?\n",
    "Para acceder a una API REST, normalmente se hace una solicitud HTTP a una URL específica (llamada endpoint). Las solicitudes más comunes son:\n",
    "\n",
    "- `GET`: para obtener datos.\n",
    "- `POST`: para enviar datos o crear nuevos recursos.\n",
    "- `PUT` y `PATCH`: para actualizar información.\n",
    "- `DELETE`: para eliminar recursos.\n",
    "\n",
    "Estas solicitudes se pueden hacer desde herramientas como `curl`, `Postman`, o directamente desde Python usando la librería `requests`.\n",
    "\n",
    "### Formato de respuesta\n",
    "La mayoría de las APIs REST devuelven los datos en formato JSON (JavaScript Object Notation), que es ligero, fácil de leer y de procesar en Python.\n",
    "\n",
    "Ejemplo de una respuesta JSON:\n",
    "{\n",
    "  \"comuna\": \"Providencia\",\n",
    "  \"poblacion\": 150000,\n",
    "  \"superficie_km2\": 14.4\n",
    "}\n",
    "\n",
    "### Autenticación y uso de API Keys\n",
    "Muchas APIs requieren autenticación para controlar el acceso. El método más simple es el uso de una API key (clave privada) que se incluye en la URL o en los headers. Algunas APIs públicas no requieren autenticación, pero otras pueden usar tokens temporales, autenticación básica o flujos más complejos como OAuth 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ_s8ZSA62i_"
   },
   "source": [
    "### Extracción de datos con una API.\n",
    "En este ejemplo usaremos una REST API pública: **OpenWeatherMap**, para obtener el clima actual de una ciudad. Para obtener la `API KEY` propia, es necesario registrarse gratis [aquí](https://openweathermap.org/price#:~:text=Learn%20more-,Free%20Access,-Current%20weather%20and).\n",
    "\n",
    "No es recomendado dejar las `API KEYS` en el notebook, para evitarlo se recomienda utilizar un archivo `.env` y la librería `dotenv` para leerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgFx5R4W_N-4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from getpass import getpass\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRjUtWUD-2Cz"
   },
   "outputs": [],
   "source": [
    "os.environ[\"API_KEY\"] = getpass(\"Ingresa tu API KEY de OpenWeatherMap: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "632ddc9d"
   },
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "ciudad = \"Santiago\"\n",
    "url = f\"http://api.openweathermap.org/data/2.5/weather?q={ciudad}&appid={API_KEY}&units=metric&lang=es\"\n",
    "\n",
    "respuesta = requests.get(url)\n",
    "datos = respuesta.json()\n",
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dad75e9a"
   },
   "source": [
    "\n",
    "Podemos inspeccionar el objeto JSON devuelto por la API y extraer información relevante.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c940d20"
   },
   "outputs": [],
   "source": [
    "\n",
    "info_clima = {\n",
    "    \"Ciudad\": datos[\"name\"],\n",
    "    \"Temperatura (°C)\": datos[\"main\"][\"temp\"],\n",
    "    \"Sensación térmica (°C)\": datos[\"main\"][\"feels_like\"],\n",
    "    \"Temperatura mínima (°C)\": datos[\"main\"][\"temp_min\"],\n",
    "    \"Temperatura máxima (°C)\": datos[\"main\"][\"temp_max\"],\n",
    "    \"Humedad (%)\": datos[\"main\"][\"humidity\"],\n",
    "    \"Presión (hPa)\": datos[\"main\"][\"pressure\"],\n",
    "    \"Velocidad del viento (m/s)\": datos[\"wind\"][\"speed\"],\n",
    "    \"Dirección del viento (°)\": datos[\"wind\"][\"deg\"],\n",
    "    \"Descripción del clima\": datos[\"weather\"][0][\"description\"],\n",
    "    \"Nubosidad (%)\": datos[\"clouds\"][\"all\"],\n",
    "    \"Visibilidad (m)\": datos[\"visibility\"]\n",
    "}\n",
    "\n",
    "pd.DataFrame([info_clima]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b80031df"
   },
   "source": [
    "\n",
    "### ¿Por qué usar APIs?\n",
    "\n",
    "- Son más confiables que el scraping. Basta que cambien aspectos mínimos de una página web para dejar obsoleto un scrapper.\n",
    "- Datos actualizados directamente desde la fuente.\n",
    "- El formato suele estar pensado para el consumo de datos.\n",
    "- Permiten acceder a grandes volúmenes de datos de forma controlada, escalable y organizada.\n",
    "- Muchas permiten filtros, autenticación y personalización de respuestas.\n",
    "\n",
    "### Buenas prácticas:\n",
    "- Nunca subir tus credenciales o API keys a repositorios públicos. Utilizar un archivo `.env` y la librería `dotenv` para leerlas.\n",
    "- Consultar la documentación oficial de cada API.\n",
    "- Limitar la frecuencia de tus consultas (respetar el \"rate limit\").\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "5f0c199c1b13c591dc8d2398e4aa6037b22139c5b858b148853eacad2edb87e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
